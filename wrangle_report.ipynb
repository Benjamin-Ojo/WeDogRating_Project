{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WeRateDog Data Wrangling Report\n",
    "---\n",
    "\n",
    "### By: Benjamin Adekunle Ojo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   >This report is to give a detailed explanation of the data gathering we under take duting our wrangling stage of our data. For our data wrangling stage of our project the following processes were taken to ensure that our data is clean and tired.\n",
    "   > \n",
    "   > * Gethering data\n",
    "   > * Accessing data \n",
    "   > * Cleaning data\n",
    "   > \n",
    "   > My data wrangling report will be following the above bullet point, breaking down each stage and explaning steps taken in other to achieve the project objectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gethering data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For this project we needed three dataset, which we had to gether using three different methods. \n",
    "> \n",
    ">  1. `Archive` Dataset: This contains information about individual tweets from weratedog tweeter account. This dataset was provided by udacity, in other to access it i was to download it alongside my project template,and using pd.read_csv to import it into my project. \n",
    ">\n",
    ">  2. `Tweet_json` or `favourite_retweet` Dataset: for this dataset I was task to extract tweet likes and retweet counts using Tweeter API, Tweepy, and tweet_id column from the archive dataset. \n",
    ">\n",
    ">  3. `Image_predication` Dataset: The image_predicaiton dataset was a dataset provided by udacity, but in other to get it we have to use a request function and url link to extact a text file, converting the text file to a tvs file before finally importing the dataset using pd.tsv. Contained in dataset are tweet_id, prediction on dog breed, confidence level of predication, and whether the image is a dog or other objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Upon gathering the dataset using various methods, I moved to the assessment phase. \n",
    ">\n",
    "> For this phase I used two methods to assess my data visual and programmatic assessment, to visually assess my data I checked every dataset for possible quality and tidiness issues, and for my programmatic assessment i used the following pandas fuctions head(), sample(), info(), unique(), and nunique() to idetify for quality and tired issues. \n",
    "> \n",
    "> The following quality and tired issues were identify during the course of our assessment, and there are as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issuses: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Archive`dataset: \n",
    ">\n",
    ">1. The url in the  `source` column needs to be extracted or delete otther stricks that are part of the url. \n",
    ">2. Convertint the `tweet_id`,`image_predication.tweet_id`, and `tweet_json.tweeted_id` from int to objects since this is not a value that we. can perform math operation on and make any sense after. \n",
    ">3. Converting the formate for the `timestamp` and `retweeted_status_timestamp` from object to datetime formate.\n",
    ">4. Cleaning incorrect rating from rating column (`rating_denominator`, `rating_numerator`)\n",
    ">5. Removing the string 'html' tag from the expanded url and source url. \n",
    ">6. Replace the 'None' string from the dog stage columns of `doggo`, `floofer`, `pupper`, and `poppo`\n",
    ">7. Removing the retweet columns and rows where retweet is true or is not null. \n",
    "\n",
    "`Image_predication` dataset: \n",
    ">\n",
    ">1. Renaming the columns without properal name description.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issues: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">1. Combining the four dog stage(`doggo`, `floofer`, `pupper`, `puppo`) into one column called Dog Stage.\n",
    ">\n",
    ">2. Joining the three dataset into one using the`tweet_id` column as the primary key to join the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The first step i took in cleaning the data was to first clean the quality issues before moving to the tidy issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Issues \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #1: \n",
    ">\n",
    ">The `source` column from the archive dataset contains links to each tweet source, but the url link was contained in some other strings that were not needed. \n",
    ">\n",
    ">So, using the str manipulation function of split we separated the strings on '\\' , this split the text into different column of which the column we need was the 1, and use this with the `.apply()` and `lambda` function we were able to separete the url from other string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #2: \n",
    ">\n",
    "> Using the `astype()` function i was able to convert the `tweet_id`,`image_predication.tweet_id`, and `tweet_json.tweeted_id`columns from int to str. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #3: \n",
    "> I found that some columns that were represented as object should have being datetime datatype, using the `pd.to_datetime` and `pd.datetim`e function i was able to cnvert them to the approprate data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #4: \n",
    "> \n",
    "> Using the `regex` and `str.contain()` function i was able to extract the rating values that are incorrect and replace them with values extracted from the `text` column from the archive dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #5: \n",
    "> \n",
    "> Using the str `.split()` function i was able to remove the 'http' string from the `source` and `expanded` column as this charater don't add any significant effect to our url link. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #6: \n",
    "> \n",
    "> During our assessment we discovered that the various dog_stage columns `doggo`, `puppo`, `floofer`, and `poppo` contains none values but instead of a python None value the values were string called 'None'.\n",
    "> \n",
    "> In other to correct this error I made use of the `np.nan` numpy function and `.replace()` fuction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #7: \n",
    "> \n",
    "> Some columns from the `image_prediction` dataset don't have properly names that describe the column, using the `rename()` fuction I was able to replace the columns name with something more meaningful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issue #8: \n",
    "> \n",
    "> The objective of the project was to analysis the tweet of weratedog tweeter account, the retweet columns and tweets that are retweet are not significant to our project and should be removed. Using the `isnull()` and `str.contain()` function i was able to remove all retweets from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidiness Issues\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidy Issue #1: \n",
    "> \n",
    "> I used `df.merge` to merge the `archive`, `tweet_json`, and `image_prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidy Issue #2: \n",
    "> \n",
    "> Using the replace concat function i was able to joint the four dog stage columns into one column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
